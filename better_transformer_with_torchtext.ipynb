{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeSaUi/DLtest/blob/main/better_transformer_with_torchtext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxEP3lOmiFsd"
      },
      "source": [
        "#Load torchtext and initialize XLM-R model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6P5DEMVJSHrD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "\n",
        "from torchtext.models import RobertaClassificationHead\n",
        "from torchtext.functional import to_tensor\n",
        "\n",
        "xlmr_large = torchtext.models.XLMR_LARGE_ENCODER\n",
        "classifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\n",
        "model = xlmr_large.get_model(head=classifier_head)\n",
        "\n",
        "# Put model into inference mode (reduces runtime even without BT - esp for GPU execution, required for Better Transformer)\n",
        "model.eval()\n",
        "\n",
        "# Define input transform\n",
        "transform = xlmr_large.transform()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System Information"
      ],
      "metadata": {
        "id": "_i8J9wc_kDaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "cpu = platform.processor()\n",
        "gpu = torch.cuda.get_device_name(DEVICE)\n",
        "\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torch cuda available: {torch.cuda.is_available()}\")\n",
        "print(f\"CPU type: {cpu}\")\n",
        "print(f\"GPU type: {gpu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mJeGWu7kKCf",
        "outputId": "1c5bf7c1-5bf1-4eb4-c5b7-e805a7572afd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.0.1+cu118\n",
            "torch cuda available: True\n",
            "CPU type: x86_64\n",
            "GPU type: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check default sparsity support setting\n",
        "Sparsity support enables transformers to skip padding in inputs."
      ],
      "metadata": {
        "id": "EY-1QDY2nS52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq61w_dQndlC",
        "outputId": "c5852484-4c3e-4a86-9546-8f5a6eaa7c8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark setup"
      ],
      "metadata": {
        "id": "TUokmFVhnvfK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqFcmAcaiDgE"
      },
      "source": [
        "###Define inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w2ETItuybcwR"
      },
      "outputs": [],
      "source": [
        "small_input_batch = [\n",
        "               \"Hello world\",\n",
        "               \"How are you!\"\n",
        "]\n",
        "big_input_batch = [\n",
        "               \"Hello world\",\n",
        "               \"How are you!\",\n",
        "               \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\n",
        "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
        "if you still try to defend the infamies and horrors perpetrated by\n",
        "that Antichrist- I really believe he is Antichrist- I will have\n",
        "nothing more to do with you and you are no longer my friend.\n",
        "\"\"\"\n",
        "\n",
        "#               `Well, Prince, so Genoa and Lucca are now just family estates of the\n",
        "#Buonapartes. But I warn you, if you don't tell me that this means war,\n",
        "#if you still try to defend the infamies and horrors perpetrated by\n",
        "#that Antichrist- I really believe he is Antichrist- I will have\n",
        "#nothing more to do with you and you are no longer my friend, no longer\n",
        "#my 'faithful slave,' as you call yourself! But how do you do? I see\n",
        "#I have frightened you- sit down and tell me all the news.`\n",
        "\n",
        "#It was in July, 1805, and the speaker was the well-known Anna\n",
        "#Pavlovna Scherer, maid of honor and favorite of the Empress Marya\n",
        "#Fedorovna. With these words she greeted Prince Vasili Kuragin, a man\n",
        "#of high rank and importance, who was the first to arrive at her\n",
        "#reception. Anna Pavlovna had had a cough for some days. She was, as\n",
        "#she said, suffering from la grippe; grippe being then a new word in\n",
        "#St. Petersburg, used only by the elite.\"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rPLNTKXiAZ4"
      },
      "source": [
        "###Select small or big input set\n",
        "\n",
        "Modify the assignment to input_batch below to select either the small_input_batch or big_inoput_batch, or substitute your own inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-21X4muzWbuX",
        "outputId": "8fce974c-388e-45c8-943d-febc979034ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "input_batch=big_input_batch\n",
        "\n",
        "model_input = to_tensor(transform(input_batch), padding_value=1)\n",
        "output = model(model_input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXSw69v4hxe2"
      },
      "source": [
        "###Iteration count for performance measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HF3OFvIdbYnM"
      },
      "outputs": [],
      "source": [
        "ITERATIONS=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAHcLbYihta8"
      },
      "source": [
        "#Measure CPU  performance with slow and fast path, without and with sparsity\n",
        "\n",
        "Sparsity support enables transformers to skip padding in inputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU performance without BT sparsity"
      ],
      "metadata": {
        "id": "8_td53nHgdQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = False"
      ],
      "metadata": {
        "id": "8aItZ7EeghTT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLZsAR15Wkgv",
        "outputId": "f2425bb6-3018-48a0-84fe-150dc3b1939c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                 aten::addmm        66.90%       19.966s        68.39%       20.411s      27.582ms           740  \n",
            "                                    aten::mm        22.18%        6.619s        22.18%        6.619s      27.579ms           240  \n",
            "                                  aten::gelu         2.72%     810.839ms         2.72%     810.839ms       3.378ms           240  \n",
            "                                 aten::copy_         2.57%     767.710ms         2.57%     767.710ms     350.553us          2190  \n",
            "                                   aten::bmm         2.00%     597.387ms         2.00%     597.430ms       1.245ms           480  \n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 29.843s\n",
            "\n",
            "fast path:\n",
            "==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
            "  return torch._transformer_encoder_layer_fwd(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             aten::addmm        37.60%       10.823s        37.80%       10.880s      21.760ms           500  \n",
            "                 aten::_addmm_activation        30.20%        8.691s        32.24%        9.280s      38.666ms           240  \n",
            "                                aten::mm        22.48%        6.471s        22.48%        6.471s      26.964ms           240  \n",
            "                   aten::_masked_softmax         2.45%     704.373ms         2.46%     706.848ms       2.945ms           240  \n",
            "       aten::_transform_bias_rescale_qkv         1.86%     535.882ms         1.91%     549.817ms       2.291ms           240  \n",
            "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 28.783s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CPU performance with BT sparsity"
      ],
      "metadata": {
        "id": "PpIGGS5egqRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = True"
      ],
      "metadata": {
        "id": "UdPM9U1Qg5y0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=False) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq4_0E4-gzwV",
        "outputId": "682229e3-7343-49ae-d70e-27d8718f8349"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                 aten::addmm        68.98%       20.622s        69.61%       20.812s      28.124ms           740  \n",
            "                                    aten::mm        22.69%        6.782s        22.69%        6.782s      28.260ms           240  \n",
            "                                   aten::bmm         1.96%     586.740ms         1.96%     586.761ms       1.222ms           480  \n",
            "                                  aten::gelu         1.67%     499.206ms         1.67%     499.206ms       2.080ms           240  \n",
            "                                 aten::copy_         1.40%     419.243ms         1.40%     419.243ms     191.435us          2190  \n",
            "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 29.896s\n",
            "\n",
            "fast path:\n",
            "==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:296: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                aten::addmm        34.25%        4.763s        34.42%        4.786s       9.572ms           500  \n",
            "                    aten::_addmm_activation        27.54%        3.830s        29.20%        4.061s      16.920ms           240  \n",
            "                                   aten::mm        20.58%        2.861s        20.58%        2.861s      11.922ms           240  \n",
            "          aten::_transform_bias_rescale_qkv         3.86%     536.441ms         6.07%     843.539ms       3.515ms           240  \n",
            "                                  aten::bmm         3.55%     494.317ms         3.56%     494.346ms       1.030ms           480  \n",
            "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 13.905s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBCDGjd-g2ny"
      },
      "source": [
        "#Measure DEVICE performance with slow and fast path, without and with sparsity\n",
        "\n",
        "Please ensure that the runtime has GPUs enabled to see the performance benefits of Better Transformer fastpath execution on GPUs. You can confirm and change the Runtime type in the Google Colab menu with (Runtime > Change Runtime Type)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "model_input = model_input.to(DEVICE)"
      ],
      "metadata": {
        "id": "bEMADUHesGjo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEVICE performance without BT sparsity"
      ],
      "metadata": {
        "id": "rcLpVrBdhEeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor=False"
      ],
      "metadata": {
        "id": "bMlwVTzfhOs-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7hMbLFbWwPH",
        "outputId": "5ed95768-1e24-419e-f71d-c9febaeae212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                               aten::mm         2.08%      83.077ms        80.17%        3.204s      13.349ms        3.306s        78.66%        3.306s      13.776ms           240  \n",
            "                                            aten::addmm         1.74%      69.481ms         2.44%      97.386ms     131.603us     374.939ms         8.92%     382.236ms     516.535us           740  \n",
            "                                              aten::bmm         0.48%      19.285ms         0.63%      25.099ms      52.290us      33.977ms         0.81%      33.977ms      70.785us           480  \n",
            "                                aten::native_layer_norm         1.38%      55.098ms         1.80%      71.750ms     146.429us      31.947ms         0.76%      45.775ms      93.418us           490  \n",
            "                                            aten::empty         0.71%      28.264ms         0.84%      33.758ms      10.582us      27.451ms         0.65%      27.451ms       8.605us          3190  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.996s\n",
            "Self CUDA time total: 4.203s\n",
            "\n",
            "fast path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm         4.21%      24.859ms         5.68%      33.565ms      67.130us     205.863ms        31.05%     206.875ms     413.750us           500  \n",
            "                                aten::_addmm_activation         2.83%      16.703ms         4.22%      24.907ms     103.779us     169.214ms        25.52%     178.050ms     741.875us           240  \n",
            "                                               aten::mm         1.33%       7.856ms         1.99%      11.772ms      49.050us     127.015ms        19.16%     127.015ms     529.229us           240  \n",
            "                                              aten::bmm         1.37%       8.087ms         1.80%      10.635ms      22.156us      20.494ms         3.09%      20.494ms      42.696us           480  \n",
            "                     aten::_native_multi_head_attention        18.05%     106.592ms        45.77%     270.328ms       1.126ms      20.088ms         3.03%     277.480ms       1.156ms           240  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 590.627ms\n",
            "Self CUDA time total: 663.081ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XC23NVChSYG"
      },
      "source": [
        "### DEVICE performance performance with BT sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d4E54ia6YYkc"
      },
      "outputs": [],
      "source": [
        "model.encoder.transformer.layers.enable_nested_tensor = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKPGx-3zaGXn",
        "outputId": "f0305b12-417b-43bc-f63f-8743d1879eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm         9.01%      47.495ms        12.17%      64.170ms      86.716us     357.134ms        47.46%     360.866ms     487.657us           740  \n",
            "                                               aten::mm         1.81%       9.528ms         2.45%      12.931ms      53.879us     116.679ms        15.51%     116.679ms     486.163us           240  \n",
            "                                              aten::bmm         2.59%      13.666ms         3.18%      16.765ms      34.927us      23.560ms         3.13%      23.560ms      49.083us           480  \n",
            "                                aten::native_layer_norm         7.44%      39.249ms         9.63%      50.800ms     103.673us      20.788ms         2.76%      29.773ms      60.761us           490  \n",
            "                                            aten::copy_         2.33%      12.304ms         3.67%      19.335ms      19.933us      16.962ms         2.25%      16.962ms      17.487us           970  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 527.419ms\n",
            "Self CUDA time total: 752.482ms\n",
            "\n",
            "fast path:\n",
            "==========\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::addmm         4.50%      39.058ms         6.44%      55.888ms      75.524us     179.201ms        20.19%     183.875ms     248.480us           740  \n",
            "                                aten::_addmm_activation         1.83%      15.892ms         2.80%      24.349ms     101.454us      88.187ms         9.94%      92.966ms     387.358us           240  \n",
            "                                            aten::empty         2.38%      20.678ms         2.42%      20.980ms       1.236us      73.823ms         8.32%      73.823ms       4.350us         16970  \n",
            "                                        aten::transpose         9.90%      85.979ms        19.91%     172.877ms      47.494us      56.291ms         6.34%     164.192ms      45.108us          3640  \n",
            "                                     aten::index_select         6.31%      54.806ms         6.58%      57.113ms      10.776us      49.545ms         5.58%      69.823ms      13.174us          5300  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 868.491ms\n",
            "Self CUDA time total: 887.455ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.to(DEVICE)\n",
        "model_input = model_input.to(DEVICE)\n",
        "\n",
        "print(\"slow path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  for i in range(ITERATIONS):\n",
        "    output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n",
        "\n",
        "print(\"fast path:\")\n",
        "print(\"==========\")\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "  with torch.no_grad():\n",
        "    for i in range(ITERATIONS):\n",
        "      output = model(model_input)\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}