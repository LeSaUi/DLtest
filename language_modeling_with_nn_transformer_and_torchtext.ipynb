{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeSaUi/DLtest/blob/main/language_modeling_with_nn_transformer_and_torchtext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fPsKmSuQ25_9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, ntoken:int, d_model:int, nhead:int, d_hid:int, nlayers:int, dropout:float=0.5):\n",
        "    super().__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "    self.embedding = nn.Embedding(ntoken, d_model)\n",
        "    self.d_model = d_model\n",
        "    self.linear = nn.Linear(d_model, ntoken)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self) -> None:\n",
        "    initrange = 0.1\n",
        "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.linear.bias.data.zero_()\n",
        "    self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "  def forward(self, src:Tensor, src_mask:Tensor = None) -> Tensor:\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "      src:Tensor, shape ''[seq_len, batch_size]''\n",
        "      src_mask:Tensor, shape ''[seq_len, seq_len]''\n",
        "\n",
        "    Returns:\n",
        "      output Tensor of shape ''[seq_len, batch_size, ntoken]''\n",
        "    \"\"\"\n",
        "    src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "    src = self.pos_encoder(src)\n",
        "    output = self.transformer_encoder(src, src_mask)\n",
        "    output = self.linear(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DyNe5ZVBC2Yg"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model:int, dropout:float = 0.1, max_len:int = 5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x:Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "      x:Tensor, shape ''[seq_len, batch_size, embedding_dim]''\n",
        "    \"\"\"\n",
        "    x = x +self.pe[:x.size(0)]\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkxGYbnAGZmb",
        "outputId": "f9d6b9fc-21db-4b12-fbda-db89f8f6deeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (1.26.16)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (16.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchdata) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ],
      "source": [
        "# To load and batch data wikitext-2 dataset, follow https://github.com/pytorch/data\n",
        "!pip install portalocker torchdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D_LicJL4LvX5"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter:dataset.IterableDataset) -> Tensor:\n",
        "  \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# ''train_iter'' was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz:int) -> Tensor:\n",
        "  \"\"\"Divides the data into ''bsz'' separate sequences, removing extra elements\n",
        "  that wouldn't cleanly fit.\n",
        "\n",
        "  Arguments:\n",
        "    data:Tensor, shape ''[N]''\n",
        "    bsz:int, batch size\n",
        "\n",
        "  Returns:\n",
        "    Tensor of shape ''[N // bsz, bsz]''\n",
        "  \"\"\"\n",
        "  seq_len = data.size(0) // bsz\n",
        "  data = data[:seq_len * bsz]\n",
        "  data = data.view(bsz, seq_len).t().contiguous()\n",
        "  return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size) # shape ''[seq_len, batch_size]''\n",
        "val_data = batchify(val_data, batch_size)\n",
        "test_data = batchify(test_data, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dwwl876OSu7T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydf_jWIsP0wb",
        "outputId": "5136266b-d070-45e4-f922-26664bf278b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2311,  0.7249,  1.4049, -0.4784],\n",
            "        [-0.3348,  1.3535, -0.9371,  0.1464],\n",
            "        [-1.3896, -0.5821, -1.1839,  1.1361]])\n",
            "tensor([[ 1.2311, -0.3348, -1.3896],\n",
            "        [ 0.7249,  1.3535, -0.5821],\n",
            "        [ 1.4049, -0.9371, -1.1839],\n",
            "        [-0.4784,  0.1464,  1.1361]])\n",
            "tensor([[ 1.2311, -0.3348, -1.3896],\n",
            "        [ 0.7249,  1.3535, -0.5821],\n",
            "        [ 1.4049, -0.9371, -1.1839],\n",
            "        [-0.4784,  0.1464,  1.1361]])\n",
            "tensor([[ 1.2311,  0.7249,  1.4049, -0.4784],\n",
            "        [-0.3348,  1.3535, -0.9371,  0.1464],\n",
            "        [-1.3896, -0.5821, -1.1839,  1.1361]])\n",
            "tensor([[ 1.2311, -0.3348, -1.3896],\n",
            "        [ 0.7249,  1.3535, -0.5821],\n",
            "        [ 1.4049, -0.9371, -1.1839],\n",
            "        [-0.4784,  0.1464,  1.1361]])\n",
            "tensor([[ 1.2311,  0.7249,  1.4049, -0.4784],\n",
            "        [-0.3348,  1.3535, -0.9371,  0.1464],\n",
            "        [-1.3896, -0.5821, -1.1839,  1.1361]])\n",
            "tensor([[ 1.2311, -0.3348, -1.3896],\n",
            "        [ 0.7249,  1.3535, -0.5821],\n",
            "        [ 1.4049, -0.9371, -1.1839],\n",
            "        [-0.4784,  0.1464,  1.1361]])\n",
            "tensor([[ 1.2311,  0.7249,  1.4049, -0.4784],\n",
            "        [-0.3348,  1.3535, -0.9371,  0.1464],\n",
            "        [-1.3896, -0.5821, -1.1839,  1.1361]])\n",
            "tensor([[ 1.2311,  0.7249,  1.4049, -0.4784],\n",
            "        [-0.3348,  1.3535, -0.9371,  0.1464],\n",
            "        [-1.3896, -0.5821, -1.1839,  1.1361]])\n",
            "tensor([[ 1.2311, -0.3348, -1.3896],\n",
            "        [ 0.7249,  1.3535, -0.5821],\n",
            "        [ 1.4049, -0.9371, -1.1839],\n",
            "        [-0.4784,  0.1464,  1.1361]])\n",
            "tensor([[ 1.2311,  0.7249,  1.4049, -0.4784],\n",
            "        [-0.3348,  1.3535, -0.9371,  0.1464],\n",
            "        [-1.3896, -0.5821, -1.1839,  1.1361]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.randn(3,4)\n",
        "print(a)\n",
        "print(a.transpose_(0,1)) # inplace true\n",
        "print(a)\n",
        "print(a.transpose(0,1)) # inplace false\n",
        "print(a)\n",
        "print(a.t()) # inplace false\n",
        "print(a)\n",
        "print(a.t_()) # inplace true\n",
        "print(a)\n",
        "print(a.T) # a.T not exist\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zf0r_liLV58G"
      },
      "outputs": [],
      "source": [
        "bptt = 35\n",
        "def get_batch(source:Tensor, i:int) -> Tuple[Tensor, Tensor]:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    source:Tensor, shape ''[full_seq_len, batch_size]''\n",
        "    i:int\n",
        "\n",
        "  Returns:\n",
        "    tuple (data, target), where data has shape ''[seq_len, batch_size]'' and\n",
        "    target has shape ''[seq_len * batch_size]''\n",
        "  \"\"\"\n",
        "  seq_len = min(bptt, len(source) - 1 - i)\n",
        "  data = source[i:i+seq_len]\n",
        "  target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "  return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uwrSHuypYvzo"
      },
      "outputs": [],
      "source": [
        "ntokens = len(vocab) # size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "d_hid = 200 # dimension of the feedforward network model in ''nn.TransformerEncoder''\n",
        "nlayers = 2 # number of ''nn.TransformerEncoderLayer'' in ''nn.TransformerEncoder''\n",
        "nhead = 2 # number of heads in ''nn.MultiheadAttention''\n",
        "dropout = 0.2 # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rautZqZKZLz1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model:nn.Module) -> None:\n",
        "  model.train() # turn on train mode\n",
        "  total_loss = 0.\n",
        "  log_interval = 200\n",
        "  start_time = time.time()\n",
        "  num_batches = len(train_data) // bptt\n",
        "\n",
        "  for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    output = model(data)\n",
        "    output_flat = output.view(-1, ntokens)\n",
        "    loss = criterion(output_flat, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if batch % log_interval == 0 and batch > 0:\n",
        "      lr = scheduler.get_last_lr()[0]\n",
        "      ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "      cur_loss = total_loss / log_interval\n",
        "      ppl = math.exp(cur_loss)\n",
        "      print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "            f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "            f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "      total_loss = 0\n",
        "      start_time = time.time()\n",
        "\n",
        "def evaluate(model:nn.Module, eval_data:Tensor) -> float:\n",
        "  model.eval() # turn on evaluation mode\n",
        "  total_loss = 0.\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "      data, targets = get_batch(eval_data, i)\n",
        "      seq_len = data.size(0)\n",
        "      output = model(data)\n",
        "      output_flat = output.view(-1, ntokens)\n",
        "      total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "  return total_loss / (len(eval_data) - 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwIpZ4u9o8EG",
        "outputId": "b9a18416-3128-46c7-f5f2-c9949ecc074e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 39.97 | loss  8.14 | ppl  3438.93\n",
            "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 16.27 | loss  6.20 | ppl   494.22\n",
            "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 17.61 | loss  5.66 | ppl   287.21\n",
            "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 22.70 | loss  5.44 | ppl   230.39\n",
            "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 16.62 | loss  5.22 | ppl   184.49\n",
            "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 17.49 | loss  4.85 | ppl   127.22\n",
            "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 16.43 | loss  4.45 | ppl    85.85\n",
            "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 19.64 | loss  4.21 | ppl    67.64\n",
            "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 15.65 | loss  3.93 | ppl    50.92\n",
            "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 14.57 | loss  3.81 | ppl    45.30\n",
            "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 15.01 | loss  3.64 | ppl    38.23\n",
            "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 18.95 | loss  3.64 | ppl    37.90\n",
            "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 15.12 | loss  3.54 | ppl    34.31\n",
            "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 16.42 | loss  3.39 | ppl    29.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 16.20 | loss  3.17 | ppl    23.70\n",
            "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 14.77 | loss  3.08 | ppl    21.76\n",
            "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 14.80 | loss  2.93 | ppl    18.65\n",
            "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 15.86 | loss  2.90 | ppl    18.15\n",
            "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 16.15 | loss  2.81 | ppl    16.60\n",
            "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 14.09 | loss  2.85 | ppl    17.27\n",
            "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 14.20 | loss  2.79 | ppl    16.33\n",
            "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 14.04 | loss  2.79 | ppl    16.30\n",
            "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 14.48 | loss  2.69 | ppl    14.68\n",
            "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 14.24 | loss  2.73 | ppl    15.36\n",
            "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 13.92 | loss  2.60 | ppl    13.52\n",
            "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 14.34 | loss  2.75 | ppl    15.68\n",
            "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 14.49 | loss  2.71 | ppl    14.98\n",
            "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 14.09 | loss  2.61 | ppl    13.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 14.01 | loss  2.50 | ppl    12.22\n",
            "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 14.65 | loss  2.47 | ppl    11.87\n",
            "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 14.58 | loss  2.48 | ppl    11.95\n",
            "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 14.02 | loss  2.43 | ppl    11.30\n",
            "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 14.05 | loss  2.32 | ppl    10.20\n",
            "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 14.71 | loss  2.41 | ppl    11.12\n",
            "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 14.49 | loss  2.34 | ppl    10.33\n",
            "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 14.10 | loss  2.48 | ppl    11.94\n",
            "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 14.09 | loss  2.45 | ppl    11.61\n",
            "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 14.06 | loss  2.39 | ppl    10.91\n",
            "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 17.30 | loss  2.32 | ppl    10.15\n",
            "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 14.66 | loss  2.44 | ppl    11.49\n",
            "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 14.07 | loss  2.49 | ppl    12.04\n",
            "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 14.15 | loss  2.42 | ppl    11.27\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "with TemporaryDirectory() as tempdir:\n",
        "  best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
        "\n",
        "  for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    scheduler.step()\n",
        "  model.load_state_dict(torch.load(best_model_params_path)) # laod best model states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VAVjFQ29I0t7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4bc41e-ff9f-4665-c3fa-6fa2798dda31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  1.34 | test ppl     3.84\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24JR3XUV1Wwy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxNJ9w+ABkDanBVSQsD6cZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}