{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9EBfHx5290NBN53VGNVc+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeSaUi/DLtest/blob/main/OPTIMIZING_VISION_TRANSFORMER_MODEL_FOR_DEPLOYMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63RoYzWbc1Y_",
        "outputId": "c61018d3-c3c5-4614-e647-ff7ce0e94ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.6.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision timm pandas requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import timm\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "\n",
        "print(torch.__version__)\n",
        "# should be 1.8.0\n",
        "\n",
        "# https://github.com/facebookresearch/deit\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=3),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
        "])\n",
        "\n",
        "img = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n",
        "img = transform(img)[None,]\n",
        "out = model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item()) # the output should be 269, which, according to the ImageNet list of class index to https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a, maps to timber wolf, grey wolf, gray wolf, Canis Iupus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exSmvLyHgoHi",
        "outputId": "d909ad4c-3512-43de-9029-0652c4d59db3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_deit_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the model on mobile, we first need to script the model. See the https://pytorch.org/tutorials/recipes/script_optimized.html\n",
        "# Run the code below to convert the DeiT model used in the previous step to the TorchScript format that can run on mobile.\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "scripted_model = torch.jit.script(model)\n",
        "scripted_model.save('fbdeit_scripted.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJt-TQdBh-sS",
        "outputId": "f52430d5-4168-4ab4-fcc4-c7dd0d7e22c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_deit_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# about quantization, refer to https://pytorch.org/docs/stable/quantization.html?highlight=quantization#dynamic-quantization\n",
        "\n",
        "# Use 'x86' for server inference (the old 'fbgemm' is still availbable but 'x86' is the recommended default) and ''qnnpack'' for mobile inference.\n",
        "backend = 'x86' # replaced with ''qnnpack'' causing much worse inference speed for quantized model on this nobtebook\n",
        "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
        "torch.backends.quantized.engine = backend\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n",
        "scripted_quantized_model = torch.jit.script(quantized_model)\n",
        "scripted_quantized_model.save('fbdeit_scripted_quantized.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnin9cq7iwGu",
        "outputId": "499d0653-59d9-4357-9408-5998c4b32c31"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = scripted_quantized_model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item()) # The same output 269 should be printed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOfpP4fhtrTm",
        "outputId": "de3edd37-defe-4aeb-d130-0e2dd8f90b64"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\n",
        "optimized_scripted_quantized_model.save('fbdeit_optimized_scripted_quantized.pt')\n"
      ],
      "metadata": {
        "id": "SfK1qhm9t4GC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = optimized_scripted_quantized_model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "print(clsidx.item()) # Again, the same output 269 should be printed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP1O-2r5vfrP",
        "outputId": "e0419a4a-ae1e-421a-ad9c-646065cb991b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_scripted_quantized_model._save_for_lite_interpreter('fbdeit_optimized_scripted_quantized_lite.ptl')\n",
        "ptl = torch.jit.load('fbdeit_optimized_scripted_quantized_lite.ptl')\n"
      ],
      "metadata": {
        "id": "OWl9c2T0vpec"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this version torch 1.8.0 profile\n",
        "uc = False\n",
        "with torch.autograd.profiler.profile(use_cuda=uc) as prof1:\n",
        "  out = model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=uc) as prof2:\n",
        "  out = scripted_model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=uc) as prof3:\n",
        "  out = scripted_quantized_model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=uc) as prof4:\n",
        "  out = optimized_scripted_quantized_model(img)\n",
        "with torch.autograd.profiler.profile(use_cuda=uc) as prof5:\n",
        "  out = ptl(img)\n",
        "\n",
        "print(f'original model: {prof1.self_cpu_time_total/1000:.2f}ms')\n",
        "print(f'scripted model: {prof2.self_cpu_time_total/1000:.2f}ms')\n",
        "print(f'scripted & quantized model: {prof3.self_cpu_time_total/1000:.2f}ms')\n",
        "print(f'scripted & quantized & optimized model: {prof4.self_cpu_time_total/1000:.2f}.ms')\n",
        "print(f'scripted & quantized & optimized & lite model: {prof5.self_cpu_time_total/1000:.2f}ms')\n",
        "\n",
        "\n",
        "\n",
        "# The result running on a Google Colab are:\n",
        "# original model: 1236.69ms\n",
        "# scripted model: 1226.72ms\n",
        "# scripted & quantized model: 593.19ms\n",
        "# scripted & quantized & optimized model: 598.01ms\n",
        "# lite model: 600.72ms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z30zGFpBwGDr",
        "outputId": "aa518bc3-f1b2-4b5f-9fba-fe7fb8d4c5cf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original model: 649.09ms\n",
            "scripted model: 667.20ms\n",
            "scripted & quantized model: 400.80ms\n",
            "scripted & quantized & optimized model: 366.27.ms\n",
            "scripted & quantized & optimized & lite model: 399.75ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this version torch 2.0.0 profile. refer https://tutorials.pytorch.kr/recipes/recipes/profiler_recipe.html\n",
        "\n",
        "uc = [torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA]\n",
        "with torch.profiler.profile(activities=uc) as pf1:\n",
        "  out = model(img)\n",
        "with torch.profiler.profile(activities=uc) as pf2:\n",
        "  out = scripted_model(img)\n",
        "with torch.profiler.profile(activities=uc) as pf3:\n",
        "  out = scripted_quantized_model(img)\n",
        "with torch.profiler.profile(activities=uc) as pf4:\n",
        "  out = optimized_scripted_quantized_model(img)\n",
        "with torch.profiler.profile(activities=uc) as pf5:\n",
        "  out = ptl(img)\n",
        "\n",
        "# if you want to see cuda profile, convert model to cuda\n",
        "# this example model maybe not cuda\n",
        "print(pf1.key_averages().table(\n",
        "    sort_by=\"cuda_time_total\", row_limit=3)) # row_limit = -1 means print all NN layer\n",
        "print(pf2.key_averages().table(\n",
        "    sort_by=\"cpu_time_total\", row_limit=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjTKoH_z0eDp",
        "outputId": "1f8e8a3c-0b22-446a-f8ca-e69ec027dc62"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                aten::conv2d         0.00%      11.000us         1.37%       8.929ms       8.929ms             1  \n",
            "           aten::convolution         0.01%      54.000us         1.37%       8.918ms       8.918ms             1  \n",
            "          aten::_convolution         0.00%      22.000us         1.36%       8.864ms       8.864ms             1  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 651.570ms\n",
            "\n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     forward         0.34%       2.306ms       100.00%     676.690ms     676.690ms             1  \n",
            "                <forward op>         0.20%       1.386ms        98.49%     666.474ms      25.634ms            26  \n",
            "                aten::linear         0.15%       1.019ms        85.98%     581.821ms      11.874ms            49  \n",
            "----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 676.713ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7ns7y3q5Mxtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame({'Model': ['original model', 'scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'scripted & quantized & optimized & lite model']})\n",
        "df = pd.concat([df, pd.DataFrame([\n",
        "    [f'{prof1.self_cpu_time_total / 1000:.2f}ms', '0%'],\n",
        "    [f'{prof2.self_cpu_time_total / 1000:.2f}ms',\n",
        "     f'{(prof1.self_cpu_time_total - prof2.self_cpu_time_total) / prof1.self_cpu_time_total * 100}'],\n",
        "    [f'{prof3.self_cpu_time_total / 1000:.2f}ms',\n",
        "     f'{(prof1.self_cpu_time_total - prof3.self_cpu_time_total) / prof1.self_cpu_time_total * 100}'],\n",
        "    [f'{prof4.self_cpu_time_total / 1000:.2f}ms',\n",
        "     f'{(prof1.self_cpu_time_total - prof4.self_cpu_time_total) / prof1.self_cpu_time_total * 100}'],\n",
        "    [f'{prof5.self_cpu_time_total / 1000:.2f}ms',\n",
        "     f'{(prof1.self_cpu_time_total - prof5.self_cpu_time_total) / prof1.self_cpu_time_total * 100}']],\n",
        "    columns=['Inference Time', 'Reduction'])], axis=1)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5wI2pF_xhIn",
        "outputId": "cfc4353b-2d6c-479c-84a7-4e03cf7d716e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           Model Inference Time  \\\n",
            "0                                 original model       649.09ms   \n",
            "1                                 scripted model       667.20ms   \n",
            "2                     scripted & quantized model       400.80ms   \n",
            "3         scripted & quantized & optimized model       366.27ms   \n",
            "4  scripted & quantized & optimized & lite model       399.75ms   \n",
            "\n",
            "            Reduction  \n",
            "0                  0%  \n",
            "1  -2.789751806375079  \n",
            "2   38.25201435856353  \n",
            "3   43.57130752283967  \n",
            "4  38.414549600209526  \n"
          ]
        }
      ]
    }
  ]
}
